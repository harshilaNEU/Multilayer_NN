{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 0.6968\n",
      "Iteration: 100, Loss: -0.8819\n",
      "Iteration: 200, Loss: -2.3964\n",
      "Iteration: 300, Loss: -3.8886\n",
      "Iteration: 400, Loss: -5.4925\n",
      "Iteration: 500, Loss: -11.9798\n",
      "Iteration: 600, Loss: -56.3560\n",
      "Iteration: 700, Loss: -56.3910\n",
      "Iteration: 800, Loss: -56.3998\n",
      "Iteration: 900, Loss: -56.4051\n",
      "Layer 1: 10 neurons, Activation: relu\n",
      "Layer 2: 8 neurons, Activation: relu\n",
      "Layer 3: 8 neurons, Activation: relu\n",
      "Layer 4: 4 neurons, Activation: relu\n",
      "Layer 5: 1 neurons, Activation: sigmoid\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# CIFAR-10 class names to integer labels mapping\n",
    "label_map = {\n",
    "    'airplane': 0,\n",
    "    'automobile': 1,\n",
    "    'bird': 2,\n",
    "    'cat': 3,\n",
    "    'deer': 4,\n",
    "    'dog': 5,\n",
    "    'frog': 6,\n",
    "    'horse': 7,\n",
    "    'ship': 8,\n",
    "    'truck': 9\n",
    "}\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in sorted(os.listdir(folder)):\n",
    "        label_folder = os.path.join(folder, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue  # Process directories\n",
    "        for image_file in os.listdir(label_folder):\n",
    "            image_path = os.path.join(label_folder, image_file)\n",
    "            with Image.open(image_path) as img:\n",
    "                img = img.resize((32, 32))  # Resize to 32x32\n",
    "                img_array = np.asarray(img, dtype=np.float32) / 255.0  # Normalize pixel values\n",
    "                images.append(img_array.flatten())  # Flatten the image\n",
    "                labels.append(label_map[label])  # Convert class names to integers\n",
    "    images = np.array(images).T  # Shape: (3072, num_samples), CIFAR-10's 32x32x3 structure\n",
    "    labels = np.array(labels).reshape(1, -1)  # Shape: (1, num_samples), now as integers\n",
    "    return images, labels\n",
    "\n",
    "# Load dataset\n",
    "X_train, y_train = load_images_from_folder('data/train')\n",
    "\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def d_relu(z):\n",
    "        return np.where(z > 0, 1.0, 0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def d_sigmoid(z):\n",
    "        s = Activation.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size) * 0.1\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.params = Parameters(input_size, output_size)\n",
    "        self.activation = activation\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.a_prev = None\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        self.a_prev = a_prev\n",
    "        self.z = np.dot(self.params.weights, a_prev) + self.params.biases\n",
    "        self.a = getattr(Activation, self.activation)(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, dA, learning_rate):\n",
    "        m = self.a_prev.shape[1]\n",
    "        if self.activation == 'relu':\n",
    "            dZ = dA * Activation.d_relu(self.z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            dZ = dA * Activation.d_sigmoid(self.z)\n",
    "        dW = np.dot(dZ, self.a_prev.T) / m\n",
    "        dB = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(self.params.weights.T, dZ)\n",
    "\n",
    "        self.params.weights -= learning_rate * dW\n",
    "        self.params.biases -= learning_rate * dB\n",
    "        return dA_prev\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, input_size=3072):\n",
    "        self.layers = [\n",
    "            Layer(input_size, 10, 'relu'),\n",
    "            Layer(10, 8, 'relu'),\n",
    "            Layer(8, 8, 'relu'),\n",
    "            Layer(8, 4, 'relu'),\n",
    "            Layer(4, 1, 'sigmoid')\n",
    "        ]\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "\n",
    "    def compute_loss(self, y, y_hat):\n",
    "        m = y.shape[1]\n",
    "        loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "        return loss\n",
    "\n",
    "    def backward_propagation(self, y, y_hat, learning_rate):\n",
    "        epsilon = 1e-7\n",
    "        dA = -(y / np.clip(y_hat, epsilon, 1 - epsilon)) + ((1 - y) / np.clip(1 - y_hat, epsilon, 1 - epsilon))\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA, learning_rate)\n",
    "\n",
    "\n",
    "    def train(self, X, y, learning_rate, n_iterations):\n",
    "        epsilon = 1e-7\n",
    "        for i in range(n_iterations):\n",
    "            y_hat = self.forward_propagation(X)\n",
    "            y_hat_clipped = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "            loss = -np.mean(y * np.log(y_hat_clipped) + (1 - y) * np.log(1 - y_hat_clipped))\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration: {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "            # Call backward_propagation with y, y_hat_clipped, and learning_rate\n",
    "            self.backward_propagation(y, y_hat_clipped, learning_rate)\n",
    "\n",
    "\n",
    "    \n",
    "    def print_model(self):\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            print(f\"Layer {i}: {layer.params.weights.shape[0]} neurons, Activation: {layer.activation}\")\n",
    "\n",
    "\n",
    "# Initialize and train the network\n",
    "dnn = DeepNeuralNetwork(input_size=3072)\n",
    "dnn.train(X_train, y_train, learning_rate=0.001, n_iterations=1000)\n",
    "dnn.print_model()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
